\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{thm}{Teorema}
\newtheorem{prop}{Proposizione}

\usepackage[ruled,vlined,linesnumbered,italiano]{algorithm2e}
\SetAlFnt{\small}
\SetAlCapNameFnt{\small}
\newcommand{\captionstyle}[1]{\small{\textsf{\textbf{#1}}}}
\SetAlCapSty{captionstyle}
\SetKwFunction{UR}{Uniform\_random}
\newcommand{\asn}{\leftarrow}

\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\Rp}{\R_+}
\newcommand{\divides}{\mid}
\newcommand{\congr}{\equiv}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\tra}{^\top}
\newcommand{\chain}[1]{\{#1_n\}_{n\in\N}}
\newcommand{\indic}{\mathbb I}
\newcommand{\ev}{\mathbb E}
\newcommand{\ape}[1]{^{(#1)}}
\newcommand{\card}[1]{\left|#1\right|}

\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\emptyset}{\varnothing}
\renewcommand{\Pr}{\mathrm{Pr}}

\usepackage[shortlabels]{enumitem}

\title{Metodi Probabilistici per l'Informatica\\[1ex]\large Riepilogo delle Dimostrazioni richieste}
\author{Alessandro Clerici}

\begin{document}
\maketitle




\section{Equivalenza tra matrici primitive e matrici irriducibili aperiodiche}


\subsection{Requisiti}
\begin{prop}\label{prop:rq}
	Sia $A\in\Rp^{k\times k}$ irriducibile e sia $d$ il suo periodo. Allora per ogni $i,j\in k$ esistono $r_{i,j},q_{i,j}\in\N$ tali che $0\le r_{i,j}<d\land 0<q_{i,j}$ e:
	\begin{enumerate}[(a)]
		\item \label{elem:rq1} $\forall s\in\N\qquad a\ape{s}_{i,j}>0\implies s\congr r_{i,j}\mod d$;
		\item \label{elem:rq2} $\forall n\ge q_{i,j}\qquad a\ape{nd+r_{i,j}}_{i,j}>0$.
	\end{enumerate}
\end{prop}

\subsection{Teorema}
\begin{thm}
	Sia $A\in\Rp^{k\times k}$.
	$A$ è primitiva se e solo se $A$ è irriducibile e aperiodica.
\end{thm}
\begin{proof}
	$\Rightarrow)$
	Se $A$ è primitiva allora, per definizione, esiste $t\in\N$ tale che $A^t>0$.
	Naturalmente $A$ è irriducibile, essendo che per ogni $i,j\in k$ vale $a^t_{i,j}>0$.
	Inoltre dalla primitività segue che $A^{t+1}>0$. Dal momento che $A^t(i,i)>0$ e $A^{t+1}(i,i)>0$ esistono cicli di lunghezze $t$ e $t+1$ nel grafo rappresentato da $A$. Quindi se $d$ è il periodo di $A$ valgono $d\divides t$ e $d\divides t+1$, il che implica $d=1$.

	$\Leftarrow)$
	Se $A$ è irriducibile e il suo periodo è $d=1$, applicando la proposizione \ref{prop:rq} si ha che per ogni $i,j\in k$, $r_{i,j}=0$ e $a\ape{n1+0}_{i,j}>0$ per ogni $n\ge q_{i,j}$. Scegliendo $q:=\max_{i,j\in k}\{q_{i,j}\}$ si ha $A^q>0$. \qedhere
\end{proof}




\section{Modulo degli autovalori nelle matrici stocastiche}

\begin{thm}
	Sia $A\in\Rp^{k\times k}$ stocastica e $\mu$ autovalore di $A$. Allora $\abs{\mu}\le 1$.
\end{thm}
\begin{proof}
	\begin{align*}
		\abs{\mu}\cdot\norm{v} & = \abs{\mu}\sum_{j=1}^k\abs{v_j} = \sum_{j=1}^k\abs {\mu v_j}                  \\
		                       & = \sum_{j=1}^k\abs {(\mu v)_j} = \sum_{j=1}^k\abs {(v\tra A)_j}                \\
		                       & = \sum_{j=1}^k\abs {(v\tra A)_j} = \sum_{j=1}^k\abs {\sum_{i=1}^k v_i a_{i,j}} \\
		                       & = \sum_{j=1}^k\abs {(v\tra A)_j} = \sum_{j=1}^k \sum_{i=1}^k \abs{v_i} a_{i,j} \\
		                       & = \sum_{j=1}^k\abs {(v\tra A)_j} = \sum_{i=1}^k \abs{v_i} \sum_{j=1}^k a_{i,j} \\
		                       & = \sum_{i=1}^k \abs{v_i} = \norm{v}                                            \\
	\end{align*}
	Da cui, essendo $v\neq 0$:
	\begin{equation*}
		\abs{\mu}\leq 1 \qedhere
	\end{equation*}
\end{proof}




\section{Equivalenza tra stati ricorrenti e essenziali}


\subsection{Requisiti}
\begin{prop}\label{prop:sumtrans}
	In una catena di markov su $(S,\mu,P)$, sia $i\in S$ transiente. Allora, per ogni $i\in S$:
	\begin{equation*}
		\sum_{n\ge0}P\ape{n}(i,j)<+\infty
	\end{equation*}
\end{prop}


\subsection{Teorema}
\begin{thm}
	Sia $\chain X$ una catena di Markov definita su $(S,\mu,P)$. Sia $i\in S$.
	\begin{enumerate}[(a)]
		\item \label{elem:ricess1} se $i$ è ricorrente allora $i$ è essenziale;
		\item \label{elem:ricess2} se $i$ è essenziale e $S$ è finito, allora $i$ è ricorrente.
	\end{enumerate}
\end{thm}
\begin{proof}
	$\Rightarrow)$
	Per assurdo, sia $i$ non essenziale. Allora $i$ raggiunge uno stato $j$ fuori dalla sua classe in $m$ passi, quindi:
	\begin{equation*}
		\Pr_i(X_m=j)>0
	\end{equation*}
	Essendo $i$ ricorrente vale inoltre $\Pr(X_n=i \text{ per infiniti $n>0$})=1$.

	Ma i due eventi sono disgiunti, poiché una volta lasciata la classe di $i$ (per arrivare a $j$) questa non può più essere raggiunta. Quindi
	\begin{equation*}
		\Pr((X_n=i \text{ per infiniti $n>0$}),X_m=j) = 0
	\end{equation*}

	Ma al contempo, essendo il primo un evento certo, vale

	\begin{equation*}
		\Pr((X_n=i \text{ per infiniti $n>0$}),X_m=j) = \Pr(X_m=j)>0 \text,
	\end{equation*}
	assurdo.

	$\Leftarrow)$
	Sia $[i]$ la classe irriducibile che contiene $i$. Essendo $[i]$ una classe essenziale vale, per ogni $n\in\N$,
	\begin{equation*}
		\sum_{j\in[i]} P\ape{n}(i,j) = 1
	\end{equation*}
	Per assurdo, sia $i$ transiente. Allora in virtù della proposizione \ref{prop:sumtrans} vale, per ogni $j\in S$:
	\begin{equation*}
		\lim_{n\to+\infty} P\ape{n}(i,j) = 0 \text.
	\end{equation*}
	Ergo:
	\begin{align*}
		1 & = \sum_{j\in[i]} P\ape{n}(i,j)                    \\
		  & = \lim_{n\to+\infty} \sum_{j\in[i]} P\ape{n}(i,j) \\
		  & = \sum_{j\in[i]} \lim_{n\to+\infty} P\ape{n}(i,j) \\
		  & = \sum_{j\in[i]} 0 = 0 \text,
	\end{align*}
	assurdo. \qedhere
\end{proof}




\section{Tempo medio di rientro finito per gli stati ricorrenti}


\subsection{Requisiti}
\begin{prop}\label{prop:transconver}
	In una catena di markov su $(S,\mu,P)$, sia $j\in S$ transiente. Allora esiste $\epsilon$ tale che $0\le\epsilon<1$ e, per $n\to+\infty$:
	\begin{equation*}
		P\ape{n}(i,j) = O(\epsilon^n) \text.
	\end{equation*}
\end{prop}


\subsection{Teorema}
\begin{prop}\label{prop:ricorconv}
	In una catena di Markov su $(S,\mu,P)$, sia $i\in S$ ricorrente. Allora esiste $\epsilon$ tale che $0\le\epsilon<1$ e, per $n\to+\infty$:
	\begin{equation*}
		f\ape{n}(i,i) = O(\epsilon^n) \text.
	\end{equation*}
\end{prop}
\begin{proof}
	Definiamo una catena sull'insieme di stati $[i]$ e la matrice di transizione $\tilde P$, che rende $i$ l'unico stato assorbente:
	\begin{gather*}
		\tilde P(i,j) := \indic(j=i) \qquad\forall j\in[i] \\
		\tilde P(h,j) := P(h,j) \qquad\forall h,j\in[i], h\neq i \text.
	\end{gather*}
	Allora vale la seguente scomposizione per $f\ape{n}(i,i)$ (si tenga presente che i cammini rappresentati da tale funzione non passano per $i$ se non all'inizio e alla fine), con $n\ge2$:
	\begin{align*}
		f\ape{n}(i,i) & = \sum_{h,j\in[i]\setminus\{i\}} P(i,h)\cdot\underbrace{\tilde P\ape{n-2}(h,j)}_{O(\epsilon^n)\text{ per proposizione \ref{prop:transconver}}}\cdot P(j,i) \\
		              & = O(\epsilon^n)\cdot O(1) = O(\epsilon^n) \text. \qedhere
	\end{align*}
\end{proof}

\begin{thm}
	In una catena di Markov su $(S,\mu,P)$, sia $i\in S$ ricorrente. Allora $\ev_i(\tau_i)<+\infty$.
\end{thm}
\begin{proof}
	Per definizione e applicando la proposizione \ref{prop:ricorconv}:
	\begin{equation*}
		\ev_i(\tau_i) = \sum_{n\ge1} nf\ape{n}(i,i) = \sum_{n\ge1} nO(e^n) < +\infty
	\end{equation*}
\end{proof}




\section{Tempo medio di rientro finito per gli stati ricorrenti}


\subsection{Requisiti}
\begin{prop}\label{prop:evrandvar}
	Sia $X$ una variabile aleatoria a valori naturali. Allora
	\begin{equation*}
		\ev(X) = \sum_{n\ge0} \Pr(X>n) \text.
	\end{equation*}
\end{prop}

\begin{prop}\label{prop:ricorfii}
	Sia $i$ uno stato ricorrente in una catena di Markov. Allora $f(i,i)=1$.
\end{prop}


\subsection{Teorema}

\begin{thm}
	Ogni catena di Markov finita possiede una distribuzione stazionaria.
\end{thm}
\begin{proof}
	Si consideri una catena sulla tripla $(S,\mu,P)$. La catena possiede uno stato essenziale e quindi ricorrente. Senza perdita di generalità sia $1$ tale stato.
	Definiamo
	\begin{equation*}
		\rho_i := \sum_{n\ge0} \Pr_1(X_n=i, \tau_1>n)
	\end{equation*}

	Dimostriamo che $\pi:=\left\{\frac{\rho_i}{\ev_1(\tau_1)}\right\}$ è distribuzione stazionaria.

	Innanzitutto, si noti che $\pi$ è stocastico se e solo se $\sum_{i\in S} \rho_i=\ev_1(\tau_1)$:
	\begin{align*}
		\sum_{i\in S} \rho_i & = \sum_{i\in S}\sum_{n\ge0} \Pr_1(X_n=i,\tau_1>n) \\
		                     & = \sum_{n\ge0}\sum_{i\in S} \Pr_1(X_n=i,\tau_1>n) \\
		                     & = \sum_{n\ge0} \Pr_1(\tau_1>n) = \ev_1(\tau_1)    \\
	\end{align*}

	Dimostriamo ora che $\rho$ è autovettore sinistro di $P$ rispeto all'autovalore $1$. Di conseguenza lo sarà anche $\pi$, ottenuto dal prodotto per uno scalare. Confrontiamo ogni coordinata $j$ dei vettori $\rho$ e $\rho\tra P$.

	Per $j\ne1$, la probabilità di essere al passo $0$ in $j$ è nulla, e l'informazione sul tempo di prima entrata in $1$ si riduce al passo $n-1$:
	\begin{align*}
		\rho_j & = \sum_{n\ge0}\Pr_1(X_n=j,\tau_1>n)                                                                                 \\
		       & = \sum_{n\ge1}\Pr_1(X_n=j,\tau_1>n-1)                                                                               \\
		       & = \sum_{n\ge1}\sum_{i\in S}\Pr_1(X_n=j,X_{n-1}=i,\tau_1>n-1)                                                        \\
		       & = \sum_{n\ge1}\sum_{i\in S}\underbrace{\Pr_1(X_n=j\mid X_{n-1}=i,\tau_1>n-1)}_{=P(i,j)} \Pr_1(X_{n-1}=i,\tau_1>n-1) \\
		       & = \sum_{i\in S} P(i,j) \sum_{n\ge1} \underbrace{\Pr_1(X_{n-1}=i,\tau_1>n-1)}_{=\rho_i} = (\rho\tra P)_j
	\end{align*}

	Quanto a $j=1$, si noti innanzitutto che $\rho_1=1$. Infatti:
	\begin{equation*}
		\rho_1 = \underbrace{\Pr_1(X_0=1,\tau_1>0)}_{=1} + \sum_{n\ge1} \underbrace{\Pr_1(X_n=1,\tau_1>n)}_{=0} = 1
	\end{equation*}
	Applicando la proposizione \ref{prop:ricorfii}:
	\begin{align*}
		\rho_1 & = 1 = f(1,1) = \sum_{n\ge1} \Pr_1(\tau_1=n)                                                  \\
		       & = \sum_{n\ge1} \Pr_1(X_n=1,\tau_1>n-1)                                                       \\
		       & = \sum_{n\ge1} \sum_{i\in S} \Pr_1(X_n=1,\tau_1>n-1,X_{n-1}=i)                               \\
		       & = \sum_{n\ge1} \sum_{i\in S} \Pr_1(X_n=1\mid\tau_1>n-1,X_{n-1}=i)\Pr_1(\tau_1>n-1,X_{n-1}=i) \\
		       & = \sum_{i\in S} P(i,1) \sum_{n\ge1} \Pr_1(\tau_1>n-1,X_{n-1}=i)                              \\
		       & = \sum_{i\in S} P(i,1) \rho_i = (\rho\tra P)_1 \qedhere
	\end{align*}
\end{proof}



\section{Teorema di ergodicità}


\subsection{Requisiti}
\begin{prop}\label{prop:stocconv}
	Sia $P\in[0,1]^{k\times k}$ stocastica primitiva. Allora esiste $\epsilon$ tale che $0\le\epsilon<1$ e per ogni coppia $u,v\in[0,1]^k$ di vettori stocastici:
	\begin{equation*}
		\norm{v\tra P^n-u\tra P^n} = O(\epsilon^n)
	\end{equation*}
\end{prop}


\subsection{Teorema}

\begin{thm}
	Sia $\chain{X}$ una catena primitiva con insieme di stati $S=\{1,\dots,k\}$. Allora
	\begin{enumerate}[(a)]
		\item \label{elem:erg1} $\chain{X}$ possiede una e una sola distribuzione stazionaria $\pi$;
		\item \label{elem:erg2} $\pi=\left\{ \frac{1}{\ev_i(\tau_i)} \right\}$;
		\item \label{elem:erg3} $\chain{X}$ è ergodica e ha distribuzione limite $\pi$.
	\end{enumerate}
\end{thm}
\begin{proof}
	Sia $P$ la matrice di transizione di $\chain{X}$.

	\ref{elem:erg1} Sia $\pi$ una distribuzione stazionaria per $\chain{X}$ (ne esiste sempre una) e sia $v$ un'ipotetica seconda distribuzione. Per la proposizione \ref{prop:stocconv} vale
	\begin{align*}
		\norm{v\tra P^n-\pi\tra P^n} & \to 0        \\
		\norm{v\tra-\pi\tra}         & \to 0 \text,
	\end{align*}
	possibile solo se $v=\pi$.

	\ref{elem:erg2} Sia $\pi^{(i)}$ la distribuzione stazionaria costruita sullo stato ricorrente $i$. Poiché la distribuzione stazionaria è unica si ha, per ogni $i,j\in S$:
	\begin{equation*}
		\pi\ape{j}_i = \pi\ape{i}_i = \frac{1}{\ev_i(\tau_i)}
	\end{equation*}

	\ref{elem:erg3} Sia $\mu$ la distribuzione iniziale di $\chain{X}$. Per la proposizione \ref{prop:stocconv} vale
	\begin{align*}
		\norm{\mu\tra P^n-\pi}                   & \to 0 \\
		\sum_{i\in S}\abs{(\mu\tra P^n)_i-\pi_i} & \to 0
	\end{align*}
	Essendo i termini della somma non negativi, questo è possibile solo se, per ogni $i\in S$:
	\begin{equation*}
		(\mu\tra P^n)_i \to \pi_i
	\end{equation*}
	Ovvero $\Pr_{\mu}(X_n=i)\to \pi_i$. \qedhere
\end{proof}



\section{Le distribuzioni reversibili sono stazionarie}


\subsection{Teorema}

\begin{thm}
	Sia $\pi$ una distribuzione reversibile per la catena $\chain{X}$. Allora $\pi$ è stazionaria per $\chain X$.
\end{thm}
\begin{proof}
	Sia $P$ la matrice di transizione di $\chain X$ e $S$ il suo insieme degli stati. Vale:
	\begin{equation*}
		(\pi\tra P)_j = \sum_{i\in S} \pi_i P(i,j) = \sum_{i\in S} \pi_j P(j,i) = \pi_j \text. \qedhere
	\end{equation*}
\end{proof}



\section{Correttezza dell'MCMC per indipendent set}

Sia $G=(V,E)$ un grafo non orientato, con $E\ne\emptyset$, e sia $S$ l'insieme di insiemi indipendenti su $G$, ossia
\begin{equation*}
	S := \{A\subseteq V\mid \forall u,v\in A, \{u,v\}\notin A\} \text.
\end{equation*}
Sia $Z_G:=\card S$ e $\{\pi:=\frac{1}{Z_G}\}_{i\in S}$. Si consideri l'algoritmo \ref{alg:indset}.

\begin{algorithm}
	\DontPrintSemicolon
	\KwData{$G=(V,E)$,$n$}

	$A\asn\{\UR{V}\}$

	\For{$n$ volte}{
		$V\asn\UR{V}$ \;
		\If{$V\in A$}{
			$A\asn A\setminus\{v\}$ \;
		}\ElseIf{$\forall z\in A\quad \{z,v\}\notin E$}{
			$A\asn A\cup\{v\}$ \;
		}
	}
	\Return A

	\caption{Algoritmo per generazione casuale di indipendent set.}
	\label{alg:indset}
\end{algorithm}

L'algoritmo codifica la catena di Markov $\chain X$ costruita sugli stati $Z_G$ e la tabella di transizione $P$ definita come segue (denotiamo con $A\div B$ la differenza simmetrica di $A$ e $B$):
\begin{equation*}
	P(A,B):=\begin{cases}
		0                  & \qquad \card{A\div B}>1 \\
		\frac 1 k          & \qquad \card{A\div B}=1 \\
		1 - \frac{d(A)}{k} & \qquad A=B              \\
	\end{cases}
\end{equation*}
Dove $d_A$ è il numero di indipendent set vicini di $A$, ossia
\begin{equation*}
	d_A:=\card{\{C\in S\mid \card{A\div C}=1\}} \text.
\end{equation*}
Si noti che $d_A\le k$, in quanto ogni nodo determina al più un vicino indipendent set.

\begin{thm}
	La catena $\chain X$ è un algoritmo MCMC corretto per la generazione uniforme di indipendent set sul grafo $G$.
\end{thm}
\begin{proof}
	La matrice $P$ è irriducibile, infatti è sempre possibile passare da un indipendent set $A$ a uno $B$ rimuovendo prima gli elementi in $A\setminus B$ e aggiungendo poi quelli in $B\setminus A$ (viene mantenuto lo status di indipendent set in quanto sottoinsiemi di $A$ e $B$ sono anch'essi indipendent set).

	La matrice è anche aperiodica, infatti sia $\{u,v\}\in E$ (si ricorda che $E\ne\emptyset$). Allora $d_{\{u\}}<k$, in quanto l'insieme $\{u,v\}$ non è indipendente. Ergo $P(\{u\},\{u\})=1-\frac{d_{\{u\}}}{k}>0$, quindi il grafo associato alla catena possiede un cappio.

	$\chain X$ è quindi primitiva ed ergodica e possiede un'unica distribuzione stazionaria, nonché distribuzione limite. Dimostriamo che tale distribuzione è $\pi$ dimostrando che essa è reversibile per $\chain X$. Per i casi $A=B$ e $\card{A\div B}>1$ la reversibilità è ovvia. Per il caso $\card{A\div B}=1$, essendo $\pi$ uniforme la reversibilità segue dalla simmetria di $P$, dovuta alla commutatività dell'operazione di differenza simmetrica.
\end{proof}



\end{document}
